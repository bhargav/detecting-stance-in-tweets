@inproceedings{Hasan2013,
author = {Hasan, Kazi Saidul and Ng, Vincent},
booktitle = {IJCNLP},
pages = {1348--1356},
title = {{Stance Classification of Ideological Debates: Data, Models, Features, and Constraints.}},
url = {http://dblp.uni-trier.de/db/conf/ijcnlp/ijcnlp2013.html\#HasanN13},
year = {2013}
}

@article{Kiritchenko2014,
author = {Kiritchenko, Svetlana and Zhu, Xiaodan and Mohammad, Saif M.},
journal = {J. Artif. Intell. Res. (JAIR)},
pages = {723--762},
title = {{Sentiment Analysis of Short Informal Texts.}},
url = {http://dblp.uni-trier.de/db/journals/jair/jair50.html\#KiritchenkoZM14},
volume = {50},
year = {2014}
}

@article{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large num- ber of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alterna- tive to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example,we present a simplemethod for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
eprint = {1310.4546},
issn = {10495258},
journal = {Nips},
pages = {1--9},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}

@article{Mikolov2013a,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
eprint = {arXiv:1301.3781v3},
journal = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
pages = {1--12},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/pdf/1301.3781v3.pdf},
year = {2013}
}

@article{Mikolov2013b,
abstract = {Continuous space language models have re- cently demonstrated outstanding results across a variety of tasks. In this paper, we ex- amine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40\% of the questions. We demonstrate that the word vectors capture semantic regu- larities by using the vector offset method to answer SemEval-2012 Task 2 questions. Re- markably, this method outperforms the best previous systems. 1},
author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
isbn = {9781937284473},
journal = {Proceedings of NAACL-HLT},
number = {June},
pages = {746--751},
title = {{Linguistic regularities in continuous space word representations}},
url = {http://scholar.google.com/scholar?hl=en\&btnG=Search\&q=intitle:Linguistic+Regularities+in+Continuous+Space+Word+Representations\#0$\backslash$nhttps://www.aclweb.org/anthology/N/N13/N13-1090.pdf},
year = {2013}
}

@article{PranavAn,
author = {{Pranav An}, Marilyn Walker, Rob Abbott, Jean E. Fox Tree, Robeson Bowmani, Michael Minor},
title = {{Cats Rule and Dogs Drool!: Classifying Stance in Online Debate}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.298.2590}
}

@article{Schneider2013,
author = {Schneider, Jodi and Groza, Tudor and Passant, Alexandre},
month = apr,
number = {2},
pages = {159--218},
title = {{A review of argumentation for the Social Semantic Web}},
url = {http://www.researchgate.net/publication/262250106\_A\_review\_of\_argumentation\_for\_the\_Social\_Semantic\_Web},
volume = {4},
year = {2013}
}
